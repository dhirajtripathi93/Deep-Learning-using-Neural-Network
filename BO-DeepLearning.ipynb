{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Deep Learning To Detect Programming Languages\n",
    "\n",
    "**`10-17-2018`**<br>\n",
    "**`BlueOptima Data challenge - Dhiraj Tripathi, Rutgers University , Masters of IT & Analytics 2017-18`**<br>\n",
    "**`email: dhiraj.tripathi@rutgers.edu`**<br>\n",
    "\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "This notebook introduces a way to use deep learning to detect programming languages. Take the following code as an example.\n",
    "\n",
    "``` Python\n",
    "def test():\n",
    "    print(\"something\")\n",
    "\n",
    "```\n",
    "We will get an answer ```python``` if we use the program to be introduced below to detect the language of the above code, which is also the correct answer. In fact, through a preliminary test, the accuracy of the program is more than 90%.\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "Let's first have a rough idea of the project structure.\n",
    "\n",
    "**Neural_Network/resources/code/train**:\n",
    "\n",
    "This folder represents the training data.The name of each subfolder representes a programming language. There are around 100 code files in each subfolder for Java, C, Javascript, Python. The data in this folder is used to train the neural network model to identify the programming language.\n",
    "\n",
    "*The data was sourced from the links given in the problem statement*\n",
    "\n",
    "**Neural_Network/resources/code/test**:\n",
    "\n",
    "This folder represents the test data. There are around 30 files per programming language. The data in this folder will be used to test the accuracy of our neural network model.\n",
    "\n",
    "**Neural_Network/src/config.py**: \n",
    "\n",
    "Some constants used in the program\n",
    "\n",
    "**Neural_Network/src/neural_network_trainer.py**:\n",
    "\n",
    "Code used to train the model.\n",
    "\n",
    "**Neural_Network/src/detector.py**: \n",
    "\n",
    "Code used to load the model and detect the programming language.\n",
    "\n",
    "**Order of Execution** : \n",
    "\n",
    "1. Run the config.py file\n",
    "2. Run the neural_network_trainer.py file\n",
    "3. Run the detector.py file\n",
    "\n",
    "This will detect the below code set as default in the detector.py file and will tell you that the code is python:\n",
    "``` Python\n",
    "def test():\n",
    "    print(\"something\")\n",
    "```\n",
    "Ofcourse you can edit the detector.py file to detect any of the 4 programming languages i.e., Java, JS, C and Python. Just type a few lines of codes in the detector.py file and run the files in the order of execution to detect the programming language.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution:\n",
    "\n",
    "Let's start with installing the required packages using the below code in the command:\n",
    "\n",
    "```conda install -c anaconda gensim```\n",
    "\n",
    "```conda install -c conda-forge keras```\n",
    "\n",
    "``` pip install tensorflow==1.3.0 ```\n",
    "\n",
    "After installing the required packages, we have to follow the order of execution and as per the order, we will have to run the config.py as below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\GIT\\\\demos\\\\Neural_Network'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"F://GIT//demos//Neural_Network\")\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.path.dirname(os.path.abspath(\"F://GIT//demos//Neural_Network\"))\n",
    "data_dir = os.path.join(current_dir, \"F://GIT//demos//Neural_Network//resources//code\")\n",
    "\n",
    "train_data_dir = os.path.join(data_dir, \"train\") #Path to the train data\n",
    "test_data_dir = os.path.join(data_dir, \"test\") #Path to the test data\n",
    "vocab_location = os.path.join(current_dir, \"F://GIT//demos//Neural_Network//resources//vocab.txt\") \n",
    "vocab_tokenizer_location = os.path.join(current_dir, \"F://GIT//demos//Neural_Network//resources//vocab_tokenizer\")\n",
    "word2vec_location = os.path.join(current_dir, \"F://GIT//demos//Neural_Network//resources/word2vec.txt\")\n",
    "model_file_location = os.path.join(current_dir, \"F://GIT//demos//Neural_Network//resources/models/model.json\")\n",
    "weights_file_location = os.path.join(current_dir, \"F://GIT//demos//Neural_Network//resources/models//model.h5\")\n",
    "\n",
    "input_length = 500\n",
    "word2vec_dimension = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have run the config file, some global constants are already declared which are going to be used further in the script. Now let's take a look at the **neural network model**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neural_network_trainer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1. Construct Vocabulary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with importing the required packages and then writing some functions to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhira\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\dhira\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import re\n",
    "from typing import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from numpy import asarray, zeros\n",
    "import pickle\n",
    "\n",
    "from src import config\n",
    "from src.config import input_length\n",
    "\n",
    "all_languages = [\"Python\", \"C\", \"Java\", \"Javascript\", ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just need to scan all the code in resources/code/train and extract common words in it. Those common words will make up our vocabulary. Key code is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(train_data_dir):\n",
    "    vocabulary = Counter()\n",
    "    files = get_files(train_data_dir)\n",
    "    for f in files:\n",
    "        words = load_words_from_file(f)\n",
    "        vocabulary.update(words)\n",
    "\n",
    "    # remove rare words\n",
    "    min_count = 5\n",
    "    vocabulary = [word for word, count in vocabulary.items() if count >= min_count]\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the build_vocab function on the training data and see the first 20 items in the list vocab:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turn off the warning messages by clicking on the toggle button below this piece of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show_err=false; \n",
       "function code_toggle_err() {\n",
       " if (code_show_err){\n",
       " $('div.output_stderr').hide();\n",
       " } else {\n",
       " $('div.output_stderr').show();\n",
       " }\n",
       " code_show_err = !code_show_err\n",
       "} \n",
       "$( document ).ready(code_toggle_err);\n",
       "</script>\n",
       "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show_err=false; \n",
    "function code_toggle_err() {\n",
    " if (code_show_err){\n",
    " $('div.output_stderr').hide();\n",
    " } else {\n",
    " $('div.output_stderr').show();\n",
    " }\n",
    " code_show_err = !code_show_err\n",
    "} \n",
    "$( document ).ready(code_toggle_err);\n",
    "</script>\n",
    "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>.''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Java\\AsciiTest.java.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Java\\CharsetsTest.java.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Java\\Utf8Test.java.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Javascript\\download.js.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Javascript\\history.js.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Javascript\\timers.js.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Javascript\\urlify.js.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Python\\forms.py.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Python\\shlex.py.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['SPDX',\n",
       " 'License',\n",
       " 'Identifier',\n",
       " 'GPL',\n",
       " '2',\n",
       " '0',\n",
       " 'Helper',\n",
       " 'function',\n",
       " 'for',\n",
       " 'splitting',\n",
       " 'a',\n",
       " 'string',\n",
       " 'into',\n",
       " 'an',\n",
       " 'argv',\n",
       " 'like',\n",
       " 'array',\n",
       " 'include',\n",
       " 'linux',\n",
       " 'kernel']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = build_vocab(config.train_data_dir)\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the vocab_tokenizer\n",
    "\n",
    "We use Tokenizer provided by Keras to build vocab_tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_tokenizer_from_set(vocab):\n",
    "    vocab_tokenizer = Tokenizer(lower=False, filters=\"\")\n",
    "    vocab_tokenizer.fit_on_texts(vocab)\n",
    "    return vocab_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we save this vocab_tokenizer as a file, to be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocab_tokenizer(vocab_tokenizer_location, vocab_tokenizer):\n",
    "    with open(vocab_tokenizer_location, 'wb') as f:\n",
    "        pickle.dump(vocab_tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build Word Vectors\n",
    "\n",
    " Word vectors are just vectors, and each word in the vocabulary is mapped to a word vector. The basic steps are below:\n",
    "1. Load all the training data, extract those words which are in the vocabulary.\n",
    "2. Map each word into its respective number by using vocab_tokenizer.\n",
    "3. Put those numbers into Word2Vec library and obtain word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word2vec(train_data_dir, vocab_tokenizer):\n",
    "    all_words = []\n",
    "    files = get_files(train_data_dir)\n",
    "    for f in files:\n",
    "        words = load_words_from_file(f)\n",
    "        all_words.append([word for word in words if is_in_vocab(word, vocab_tokenizer)])\n",
    "    model = Word2Vec(all_words, size=100, window=5, workers=8, min_count=1)\n",
    "    return {word: model[word] for word in model.wv.index2word}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Build the Neural Network\n",
    "\n",
    "For a clear understanding let's say that input of the Neural Network is the words mapped into numbers and the output is the probability of the code to belonging to a specific programming language.\n",
    "\n",
    "Now that we know the input and output of the Neural Network, let's follow the steps below to train the model below:\n",
    "\n",
    "1. **Embedding Layer**: it’s used to map each word into its respective word vector\n",
    "2. **Conv1D, MaxPooling1D**: this part is a classic deep learning layer. To put it simply, what it does is extraction and transformation.\n",
    "3. **Flatten, Dense**: convert the multi-dimensional array into one-dimensional, and output the prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(train_data_dir, vocab_tokenizer, word2vec):\n",
    "    weight_matrix = build_weight_matrix(vocab_tokenizer, word2vec)\n",
    "\n",
    "    # build the embedding layer\n",
    "    input_dim = len(vocab_tokenizer.word_index) + 1\n",
    "    output_dim = get_word2vec_dimension(word2vec)\n",
    "    x_train, y_train = load_data(train_data_dir, vocab_tokenizer)\n",
    "\n",
    "    embedding_layer = Embedding(input_dim, output_dim, weights=[weight_matrix], input_length=input_length,\n",
    "                                trainable=False)\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, activation=\"relu\"))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(len(all_languages), activation=\"sigmoid\"))\n",
    "    logging.info(model.summary())\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=10, verbose=2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s write a function, which uses the neural network to detect test code, check out its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(test_data_dir, vocab_tokenizer, model):\n",
    "    x_test, y_test = load_data(test_data_dir, vocab_tokenizer)\n",
    "    loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    logging.info('Test Accuracy: %f' % (acc * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As what we have got before, the test accuracy is around 94%~95%, which is good enough. Let’s save the neural network as files, so we can load it when detecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_file_location, weights_file_location):\n",
    "    os.makedirs(os.path.dirname(model_file_location), exist_ok=True)\n",
    "    with open(model_file_location, \"w\") as f:\n",
    "        f.write(model.to_json())\n",
    "    model.save_weights(weights_file_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rest of the below functions are a part of the neural network and are referenced through out the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_words_from_string(s):\n",
    "    contents = \" \".join(s.splitlines())\n",
    "    result = re.split(r\"[{}()\\[\\]\\'\\\":.*\\s,#=_/\\\\><;?\\-|+]\", contents)\n",
    "\n",
    "    # remove empty elements\n",
    "    result = [word for word in result if word.strip() != \"\"]\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab_tokenizer(vocab_tokenizer_location):\n",
    "    with open(vocab_tokenizer_location, 'rb') as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_saved_data(x_file_name, y_file_name, model):\n",
    "    x = np.loadtxt(x_file_name)\n",
    "    y = np.loadtxt(y_file_name)\n",
    "    loss, accuracy = model.evaluate(x, y, verbose=2)\n",
    "    print(f\"loss: {loss}, accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_binary_list(i, count):\n",
    "    result = [0] * count\n",
    "    result[i] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lang_sequence(lang):\n",
    "    for i in range(len(all_languages)):\n",
    "        if all_languages[i] == lang:\n",
    "            return to_binary_list(i, len(all_languages))\n",
    "    raise Exception(f\"Language {lang} is not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(sentence, vocab_tokenizer):\n",
    "    encoded_sentence = vocab_tokenizer.texts_to_sequences(sentence.split())\n",
    "    return [word[0] for word in encoded_sentence if len(word) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(vocab_location):\n",
    "    with open(vocab_location) as f:\n",
    "        words = f.read().splitlines()\n",
    "    return set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word2vec(word2vec_location):\n",
    "    result = dict()\n",
    "    with open(word2vec_location, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()[1:]\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        result[parts[0]] = asarray(parts[1:], dtype=\"float32\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_file_location, weights_file_location):\n",
    "    with open(model_file_location) as f:\n",
    "        model = model_from_json(f.read())\n",
    "    model.load_weights(weights_file_location)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(data_dir):\n",
    "    result = []\n",
    "    depth = 0\n",
    "    for root, sub_folders, files in os.walk(data_dir):\n",
    "        depth += 1\n",
    "\n",
    "        # ignore the first loop\n",
    "        if depth == 1:\n",
    "            continue\n",
    "\n",
    "        language = os.path.basename(root)\n",
    "        result.extend([os.path.join(root, f) for f in files])\n",
    "        depth += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_words_from_file(file_name):\n",
    "    try:\n",
    "        with open(file_name, \"r\") as f:\n",
    "            contents = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        logging.warning(f\"Encountered UnicodeDecodeError, ignore file {file_name}.\")\n",
    "        return []\n",
    "    return load_words_from_string(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_languages(ext_lang_dict):\n",
    "    languages = set()\n",
    "    for ext, language in ext_lang_dict.items():\n",
    "        if type(language) is str:\n",
    "            languages.update([language])\n",
    "        elif type(language) is list:\n",
    "            languages.update(language)\n",
    "    return languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocabulary(vocabulary, file_location):\n",
    "    with open(file_location, \"w+\") as f:\n",
    "        for word in vocabulary:\n",
    "            f.write(word + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_vocab(word, vocab_tokenizer):\n",
    "    return word in vocab_tokenizer.word_counts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_qualified_words(words, vocab_tokenizer):\n",
    "    return \" \".join([word for word in words if is_in_vocab(word, vocab_tokenizer)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentence_from_file(file_name, vocab_tokenizer):\n",
    "    words = load_words_from_file(file_name)\n",
    "    return concatenate_qualified_words(words, vocab_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentence_from_string(s, vocab_tokenizer):\n",
    "    words = load_words_from_string(s)\n",
    "    return concatenate_qualified_words(words, vocab_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_encoded_sentence_from_file(file_name, vocab_tokenizer):\n",
    "    sentence = load_sentence_from_file(file_name, vocab_tokenizer)\n",
    "    return encode_sentence(sentence, vocab_tokenizer)\n",
    "\n",
    "\n",
    "def load_encoded_sentence_from_string(s, vocab_tokenizer):\n",
    "    sentence = load_sentence_from_string(s, vocab_tokenizer)\n",
    "    return encode_sentence(sentence, vocab_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir, vocab_tokenizer):\n",
    "    files = get_files(data_dir)\n",
    "    x = []\n",
    "    y = []\n",
    "    for f in files:\n",
    "        language = os.path.dirname(f).split(os.path.sep)[-1]\n",
    "        x.append(load_encoded_sentence_from_file(f, vocab_tokenizer))\n",
    "        y.append(get_lang_sequence(language))\n",
    "    return pad_sequences(x, maxlen=input_length), asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_dimension(word2vec):\n",
    "    first_vector = list(word2vec.values())[0]\n",
    "    return len(first_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_weight_matrix(vocab_tokenizer, word2vec):\n",
    "    vocab_size = len(vocab_tokenizer.word_index) + 1\n",
    "    word2vec_dimension = get_word2vec_dimension(word2vec)\n",
    "    weight_matrix = zeros((vocab_size, word2vec_dimension))\n",
    "    for word, index in vocab_tokenizer.word_index.items():\n",
    "        weight_matrix[index] = word2vec[word]\n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_save_vocab_tokenizer(train_data_dir, vocab_tokenizer_location):\n",
    "    vocab = build_vocab(train_data_dir)\n",
    "    vocab_tokenizer = build_vocab_tokenizer_from_set(vocab)\n",
    "    save_vocab_tokenizer(vocab_tokenizer_location, vocab_tokenizer)\n",
    "    return vocab_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Java\\AsciiTest.java.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Java\\CharsetsTest.java.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Java\\Utf8Test.java.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Javascript\\download.js.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Javascript\\history.js.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Javascript\\timers.js.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Javascript\\urlify.js.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Python\\forms.py.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Python\\shlex.py.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Java\\AsciiTest.java.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Java\\CharsetsTest.java.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Java\\Utf8Test.java.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Javascript\\download.js.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Javascript\\history.js.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Javascript\\timers.js.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Javascript\\urlify.js.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Python\\forms.py.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Python\\shlex.py.\n",
      "INFO:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:gensim.models.word2vec:collected 8712 word types from a corpus of 543284 raw words and 480 sentences\n",
      "INFO:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:gensim.models.word2vec:effective_min_count=1 retains 8712 unique words (100% of original 8712, drops 0)\n",
      "INFO:gensim.models.word2vec:effective_min_count=1 leaves 543284 word corpus (100% of original 543284, drops 0)\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 8712 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 45 most-common words\n",
      "INFO:gensim.models.word2vec:downsampling leaves estimated 472089 word corpus (86.9% of prior 543284)\n",
      "INFO:gensim.models.base_any2vec:estimated required memory for 8712 words and 100 dimensions: 11325600 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "INFO:gensim.models.base_any2vec:training model with 8 workers on 8712 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 1 : training on 543284 raw words (434894 effective words) took 0.5s, 953462 effective words/s\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 2 : training on 543284 raw words (434460 effective words) took 0.5s, 799159 effective words/s\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 3 : training on 543284 raw words (434741 effective words) took 0.4s, 1016150 effective words/s\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 4 : training on 543284 raw words (434711 effective words) took 0.4s, 1188374 effective words/s\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 5 : training on 543284 raw words (434864 effective words) took 0.4s, 1158021 effective words/s\n",
      "INFO:gensim.models.base_any2vec:training on a 2716420 raw words (2173670 effective words) took 2.2s, 988530 effective words/s\n",
      "C:\\Users\\dhira\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Java\\AsciiTest.java.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Java\\CharsetsTest.java.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Java\\Utf8Test.java.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Javascript\\download.js.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Javascript\\history.js.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Javascript\\timers.js.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Javascript\\urlify.js.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Python\\forms.py.\n",
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\train\\Python\\shlex.py.\n",
      "INFO:root:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 100)          871300    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 496, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 248, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 31744)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 126980    \n",
      "=================================================================\n",
      "Total params: 1,062,408\n",
      "Trainable params: 191,108\n",
      "Non-trainable params: 871,300\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.5713 - acc: 0.7880\n",
      "Epoch 2/10\n",
      " - 3s - loss: 0.1465 - acc: 0.9500\n",
      "Epoch 3/10\n",
      " - 3s - loss: 0.0761 - acc: 0.9818\n",
      "Epoch 4/10\n",
      " - 3s - loss: 0.0471 - acc: 0.9880\n",
      "Epoch 5/10\n",
      " - 3s - loss: 0.0350 - acc: 0.9922\n",
      "Epoch 6/10\n",
      " - 3s - loss: 0.0287 - acc: 0.9922\n",
      "Epoch 7/10\n",
      " - 3s - loss: 0.0244 - acc: 0.9922\n",
      "Epoch 8/10\n",
      " - 3s - loss: 0.0218 - acc: 0.9922\n",
      "Epoch 9/10\n",
      " - 3s - loss: 0.0196 - acc: 0.9927\n",
      "Epoch 10/10\n",
      " - 3s - loss: 0.0182 - acc: 0.9938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Encountered UnicodeDecodeError, ignore file F:\\GIT\\demos\\Neural_Network\\src\\../resources/code\\test\\Javascript\\app.js.\n",
      "INFO:root:Test Accuracy: 94.600001\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    vocab_tokenizer = build_and_save_vocab_tokenizer(config.train_data_dir, config.vocab_tokenizer_location)\n",
    "    word2vec = build_word2vec(config.train_data_dir, vocab_tokenizer)\n",
    "\n",
    "    model = build_model(config.train_data_dir, vocab_tokenizer, word2vec)\n",
    "    evaluate_model(config.test_data_dir, vocab_tokenizer, model)\n",
    "\n",
    "    save_model(model, config.model_file_location, config.weights_file_location)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### detector.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Load the Neural Network For Detection\n",
    "\n",
    "In this part, we need to load vocab_tokenizer and the neural network for detection. The code is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from src import config\n",
    "from src.config import input_length\n",
    "from src.neural_network_trainer import load_model, \\\n",
    "    load_vocab_tokenizer, load_encoded_sentence_from_string, all_languages\n",
    "\n",
    "vocab_tokenizer = load_vocab_tokenizer(config.vocab_tokenizer_location)\n",
    "model = load_model(config.model_file_location, config.weights_file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_language(binary_list):\n",
    "    i = np.argmax(binary_list)\n",
    "    return all_languages[i]\n",
    "\n",
    "\n",
    "def get_neural_network_input(code):\n",
    "    encoded_sentence = load_encoded_sentence_from_string(code, vocab_tokenizer)\n",
    "    return pad_sequences([encoded_sentence], maxlen=input_length)\n",
    "\n",
    "\n",
    "def detect(code):\n",
    "    y_proba = model.predict(get_neural_network_input(code))\n",
    "    return to_language(y_proba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\"\n",
    "   \n",
    "\n",
    "package com.google.common.base;\n",
    "\n",
    "import static com.google.common.base.Preconditions.checkNotNull;\n",
    "\n",
    "import com.google.common.annotations.Beta;\n",
    "import com.google.common.annotations.GwtCompatible;\n",
    "import java.io.Serializable;\n",
    "import java.util.Iterator;\n",
    "import java.util.Set;\n",
    "import org.checkerframework.checker.nullness.qual.Nullable;\n",
    "\n",
    "\n",
    "  public abstract T or(T defaultValue);\n",
    "\n",
    "  /**\n",
    "   * Returns this {@code Optional} if it has a value present; {@code secondChoice} otherwise.\n",
    "   *\n",
    "   * <p><b>Comparison to {@code java.util.Optional}:</b> this method has no equivalent in Java 8's\n",
    "   * {@code Optional} class; write {@code thisOptional.isPresent() ? thisOptional : secondChoice}\n",
    "   * instead.\n",
    "   */\n",
    " \n",
    "   * @throws NullPointerException if this optional's value is absent and the supplier returns {@code\n",
    "   *     null}\n",
    "   */\n",
    "  @Beta\n",
    " ct <V> Optional<V> transform(Function<? super T, V> function);\n",
    "\n",
    "  /**\n",
    "   * Returns {@code true} if {@code object} is an {@code Optional} instance, and either the\n",
    "   * contained references are {@linkplain Object#equals equal} to each other or both are absent.\n",
    "   * Note that {@code Optional} instances of differing parameterized types can be equal.\n",
    "   *\n",
    "   * <p><b>Comparison to {@code java.util.Optional}:</b> no differences.\n",
    "   */\n",
    "  @Override\n",
    "  public abstract boolean equals(@Nullable Object object);\n",
    "\n",
    " \n",
    "\n",
    "  private static final long serialVersionUID = 0;\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java\n"
     ]
    }
   ],
   "source": [
    "print(detect(code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You should be able to notice that the detected code is Java. You can replace the string \"code\" with a chunk of code from any of the four programming launguages i.e., Java, JS, C and Python. \n",
    "\n",
    "Lets test the model for some different programming language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\"\n",
    "   \n",
    "\n",
    "\n",
    "#include <linux/export.h>\n",
    "\n",
    "#include <linux/libgcc.h>\n",
    "\n",
    "long long notrace __ashldi3(long long u, word_type b)\n",
    "{\n",
    "\tDWunion uu, w;\n",
    "\tword_type bm;\n",
    "\n",
    "\tif (b == 0)\n",
    "\t\treturn u;\n",
    "\n",
    "\tuu.ll = u;\n",
    "\tbm = 32 - b;\n",
    "\n",
    "\tif (bm <= 0) {\n",
    "\t\tw.s.low = 0;\n",
    "\t\tw.s.high = (unsigned int) uu.s.low << -bm;\n",
    "\t} else {\n",
    "\t\tconst unsigned int carries = (unsigned int) uu.s.low >> bm;\n",
    "\n",
    "\t\tw.s.low = (unsigned int) uu.s.low << b;\n",
    "\t\tw.s.high = ((unsigned int) uu.s.high << b) | carries;\n",
    "\t}\n",
    "\n",
    "\treturn w.ll;\n",
    "}\n",
    "EXPORT_SYMBOL(__ashldi3);\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n"
     ]
    }
   ],
   "source": [
    "print(detect(code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "\n",
    "There are below 5 conceptual steps in training this neural network model:\n",
    "1. Build vocabulary.\n",
    "2. Build vocab_tokenizer using vocabulary, which is used to convert words into numbers.\n",
    "3. Load words into Word2Vec to build word vectors.\n",
    "4. Load word vectors into the neural network as part of the input layer.\n",
    "5. Load all the training data, extract words that are in the vocabulary, convert them into numbers using vocab_tokenizer, load them into the neural network for training.\n",
    "\n",
    "#### Three steps for detection:\n",
    "1. Extract words in the code and remove those that are not in the vocabulary.\n",
    "2. Convert those words into number through vocab_tokenizer, and load them into the neural network.\n",
    "3. Choose the language which has the most probability, which the answer we want. The input to the neural network is the number(mapped from vocab tokenizer) and the output is the probability of a code to be of the specific programming language.\n",
    "\n",
    "## Possible Enhancements:\n",
    "\n",
    "The detector demostrated above requires a manual input from the user to type in the code in the last part of the notebook to be able to detect the programming language. If given more time, I would surely like to research on the part to automate the process in which there is no need of manual input and the script automatically opens a folder, reads the file, and detects the programming language as output. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
